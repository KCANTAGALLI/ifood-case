"""
Visualizador de Resultados dos Testes
====================================

Exibe os resultados dos testes de forma amigÃ¡vel.
"""

import json
import os
from datetime import datetime

def show_test_results():
    # Encontrar o arquivo de log mais recente
    log_files = [f for f in os.listdir('.') if f.startswith('test_log_') and f.endswith('.json')]
    
    if not log_files:
        print("âŒ Nenhum arquivo de log encontrado!")
        return
    
    # Pegar o mais recente
    latest_log = sorted(log_files)[-1]
    
    print(f"ğŸ“Š RELATÃ“RIO DETALHADO DOS TESTES")
    print("=" * 60)
    print(f"ğŸ“„ Arquivo: {latest_log}")
    
    try:
        with open(latest_log, 'r', encoding='utf-8') as f:
            log = json.load(f)
        
        # InformaÃ§Ãµes da sessÃ£o
        session = log["test_session"]
        print(f"ğŸ” Session ID: {session['session_id']}")
        print(f"â° Timestamp: {session['start_time']}")
        print(f"ğŸ¯ Status Geral: {log['test_results']['summary']['overall_status'].upper()}")
        
        # Resumo dos testes
        summary = log["test_results"]["summary"]
        print(f"âœ… Testes Aprovados: {summary['passed_tests']}/{summary['total_tests']}")
        print(f"âŒ Testes Falharam: {summary['failed_tests']}")
        
        # MÃ©tricas do projeto
        print(f"\nğŸ“Š MÃ‰TRICAS DO PROJETO:")
        metrics = log["metrics"]["project_files"]
        print(f"  ğŸ“ Arquivos Python: {metrics['total_python_files']}")
        print(f"  ğŸ“ Linhas de cÃ³digo: {metrics['total_lines_of_code']:,}")
        print(f"  ğŸ—ï¸  Classes: {metrics['total_classes']}")
        print(f"  âš™ï¸  FunÃ§Ãµes: {metrics['total_functions']}")
        
        # Cobertura
        print(f"\nğŸ“‹ COBERTURA:")
        coverage = log["metrics"]["coverage"]
        files_pct = (coverage['required_files_found'] / coverage['required_files_total']) * 100
        patterns_pct = (coverage['required_patterns_found'] / coverage['required_patterns_total']) * 100
        
        print(f"  ğŸ“‚ Arquivos obrigatÃ³rios: {coverage['required_files_found']}/{coverage['required_files_total']} ({files_pct:.0f}%)")
        print(f"  ğŸ¯ PadrÃµes obrigatÃ³rios: {coverage['required_patterns_found']}/{coverage['required_patterns_total']} ({patterns_pct:.0f}%)")
        
        # Detalhes dos testes individuais
        print(f"\nğŸ§ª DETALHES DOS TESTES:")
        for i, test in enumerate(log["test_results"]["individual_tests"], 1):
            status_icon = "âœ…" if test["status"] == "passed" else "âŒ"
            print(f"  {i}. {status_icon} {test['test_name']}: {test['status'].upper()}")
            print(f"     {test['description']}")
        
        # Arquivos analisados
        print(f"\nğŸ“ ARQUIVOS ANALISADOS:")
        python_test = next(t for t in log["test_results"]["individual_tests"] if t["test_name"] == "python_files_analysis")
        
        for file_path, details in python_test["details"]["files"].items():
            if details.get("analysis_success", False):
                lines = details.get("total_lines", 0)
                classes = len(details.get("classes", []))
                functions = len(details.get("functions", []))
                print(f"  âœ… {file_path}")
                print(f"     Linhas: {lines}, Classes: {classes}, FunÃ§Ãµes: {functions}")
            else:
                print(f"  âŒ {file_path}: {details.get('error', 'Erro desconhecido')}")
        
        print(f"\n" + "=" * 60)
        if log["test_results"]["summary"]["overall_status"] == "passed":
            print("ğŸ‰ TODOS OS TESTES PASSARAM!")
            print("âœ… O projeto estÃ¡ pronto para execuÃ§Ã£o no Databricks")
        else:
            print("âš ï¸  ALGUNS TESTES FALHARAM!")
            print("âŒ Verifique os problemas reportados")
            
    except Exception as e:
        print(f"âŒ Erro ao ler arquivo de log: {e}")

if __name__ == "__main__":
    show_test_results()
