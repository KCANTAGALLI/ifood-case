{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NYC Taxi Data Analysis\n",
        "\n",
        "This notebook contains the required analytical queries for the NYC Taxi data project.\n",
        "\n",
        "## Required Analyses:\n",
        "1. Calculate average total_amount per month considering all trips\n",
        "2. Calculate average passenger_count per hour in May\n",
        "\n",
        "## Prerequisites:\n",
        "- ETL pipeline must be completed (Bronze, Silver, Gold layers)\n",
        "- Gold layer tables must be available in the metastore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session (if not already available)\n",
        "# In Databricks, spark is already available\n",
        "try:\n",
        "    spark\n",
        "    print(\"âœ… Using existing Spark session\")\n",
        "except NameError:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"NYC_Taxi_Analysis\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "    print(\"âœ… Spark session created\")\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 1: Monthly average total_amount\n",
        "monthly_query = \"\"\"\n",
        "SELECT \n",
        "    pickup_month,\n",
        "    CASE pickup_month\n",
        "        WHEN 1 THEN 'January'\n",
        "        WHEN 2 THEN 'February' \n",
        "        WHEN 3 THEN 'March'\n",
        "        WHEN 4 THEN 'April'\n",
        "        WHEN 5 THEN 'May'\n",
        "    END as month_name,\n",
        "    ROUND(avg_total_amount, 2) as avg_total_amount,\n",
        "    total_trips\n",
        "FROM gold_monthly_aggregations \n",
        "ORDER BY pickup_month\n",
        "\"\"\"\n",
        "\n",
        "monthly_df = spark.sql(monthly_query)\n",
        "monthly_pandas = monthly_df.toPandas()\n",
        "\n",
        "print(\"ðŸ“Š REQUIRED ANALYSIS 1: Monthly Average Total Amount\")\n",
        "print(\"=\" * 55)\n",
        "for _, row in monthly_pandas.iterrows():\n",
        "    print(f\"{row['month_name']:>9}: ${row['avg_total_amount']:>6.2f} (from {row['total_trips']:>8,} trips)\")\n",
        "\n",
        "# Display as Spark DataFrame\n",
        "monthly_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query 2: Hourly average passenger_count in May\n",
        "hourly_query = \"\"\"\n",
        "SELECT \n",
        "    pickup_hour,\n",
        "    CASE \n",
        "        WHEN pickup_hour = 0 THEN '12:00 AM'\n",
        "        WHEN pickup_hour < 12 THEN CONCAT(pickup_hour, ':00 AM')\n",
        "        WHEN pickup_hour = 12 THEN '12:00 PM'\n",
        "        ELSE CONCAT(pickup_hour - 12, ':00 PM')\n",
        "    END as hour_display,\n",
        "    ROUND(avg_passenger_count, 2) as avg_passenger_count,\n",
        "    total_trips\n",
        "FROM gold_hourly_aggregations_may \n",
        "ORDER BY pickup_hour\n",
        "\"\"\"\n",
        "\n",
        "hourly_df = spark.sql(hourly_query)\n",
        "hourly_pandas = hourly_df.toPandas()\n",
        "\n",
        "print(\"ðŸ“Š REQUIRED ANALYSIS 2: Hourly Average Passenger Count (May 2023)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Display key hours\n",
        "for i in range(0, len(hourly_pandas), 3):\n",
        "    row = hourly_pandas.iloc[i]\n",
        "    print(f\"{row['hour_display']:>8}: {row['avg_passenger_count']:>4.2f} passengers (from {row['total_trips']:>6,} trips)\")\n",
        "\n",
        "print(\"\\n(Full 24-hour data available in the DataFrame below)\")\n",
        "hourly_df.show(24)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
