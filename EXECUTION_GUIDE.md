# Guia de Execu√ß√£o - NYC Taxi ETL Pipeline

Este guia fornece instru√ß√µes detalhadas para executar o pipeline de dados de t√°xis de NYC.

##  Execu√ß√£o R√°pida (Databricks - Recomendado)

### 1. Prepara√ß√£o do Ambiente

1. **Acesse o Databricks Community Edition**:
   - Crie uma conta em: https://community.cloud.databricks.com/
   - Crie um cluster com Spark 3.x

2. **Upload dos Arquivos**:
   - Fa√ßa upload da pasta `src/` para o workspace do Databricks
   - Fa√ßa upload do arquivo `databricks_etl_runner.py`
   - Fa√ßa upload do notebook `analysis/NYC_Taxi_Analysis.ipynb`

### 2. Execu√ß√£o do Pipeline

**Op√ß√£o A: Usando o Runner Otimizado (Mais Simples)**
```python
# Execute o arquivo databricks_etl_runner.py no Databricks
# Ele cont√©m todas as c√©lulas necess√°rias para executar o pipeline completo
```

**Op√ß√£o B: Execu√ß√£o Manual**
```python
# C√©lula 1: Configura√ß√£o
from src.etl_pipeline import ETLPipeline

config = {
    "data_paths": {
        "bronze": "/dbfs/tmp/datalake/bronze",
        "silver": "/dbfs/tmp/datalake/silver", 
        "gold": "/dbfs/tmp/datalake/gold"
    }
}

# C√©lula 2: Executar Pipeline
pipeline = ETLPipeline()
success = pipeline.run_pipeline()

if success:
    print(" Pipeline executado com sucesso!")
else:
    print(" Pipeline falhou. Verifique os logs.")
```

### 3. Executar An√°lises Obrigat√≥rias

```python
# An√°lise 1: M√©dia mensal de total_amount
spark.sql("""
    SELECT 
        pickup_month,
        CASE pickup_month
            WHEN 1 THEN 'Janeiro'
            WHEN 2 THEN 'Fevereiro' 
            WHEN 3 THEN 'Mar√ßo'
            WHEN 4 THEN 'Abril'
            WHEN 5 THEN 'Maio'
        END as mes,
        ROUND(avg_total_amount, 2) as valor_medio
    FROM gold_monthly_aggregations 
    ORDER BY pickup_month
""").show()

# An√°lise 2: M√©dia hor√°ria de passenger_count (Maio)
spark.sql("""
    SELECT 
        pickup_hour as hora,
        ROUND(avg_passenger_count, 2) as passageiros_medio
    FROM gold_hourly_aggregations_may 
    ORDER BY pickup_hour
""").show()
```

## üñ• Execu√ß√£o Local (Alternativa)

### 1. Configura√ß√£o do Ambiente

```bash
# Clone o reposit√≥rio
git clone <repository-url>
cd ifood-case

# Instale as depend√™ncias
pip install -r requirements.txt

# Instale o PySpark (se necess√°rio)
pip install pyspark==3.4.1
```

### 2. Configura√ß√£o de Paths

Edite o arquivo `config.yaml` ou defina as vari√°veis:

```python
# Para execu√ß√£o local
config = {
    "data_paths": {
        "bronze": "/tmp/datalake/bronze",
        "silver": "/tmp/datalake/silver",
        "gold": "/tmp/datalake/gold"
    }
}
```

### 3. Executar Pipeline

```bash
# Op√ß√£o 1: Script principal
python src/etl_pipeline.py

# Op√ß√£o 2: Execu√ß√£o por camadas
python -c "
from src.etl_pipeline import ETLPipeline
pipeline = ETLPipeline()
pipeline.run_pipeline()
"

# Op√ß√£o 3: An√°lises apenas
python analysis/nyc_taxi_analysis.py
```

##  Verifica√ß√£o dos Resultados

### Tabelas Criadas

Ap√≥s a execu√ß√£o bem-sucedida, as seguintes tabelas estar√£o dispon√≠veis:

**Bronze Layer:**
- `bronze_taxi_data_2023_01` at√© `bronze_taxi_data_2023_05`

**Silver Layer:**
- `silver_taxi_trips_clean`
- `silver_quality_reports`

**Gold Layer:**
- `gold_monthly_aggregations`
- `gold_hourly_aggregations_may`
- `gold_vendor_analysis`
- `gold_weekend_analysis`
- `gold_business_kpis`

### Consultas de Verifica√ß√£o

```sql
-- Verificar tabelas criadas
SHOW TABLES;

-- Verificar dados Bronze
SELECT COUNT(*) FROM bronze_taxi_data_2023_01;

-- Verificar dados Silver
SELECT COUNT(*) FROM silver_taxi_trips_clean;

-- Verificar dados Gold
SELECT * FROM gold_monthly_aggregations;
SELECT * FROM gold_hourly_aggregations_may LIMIT 10;
```

##  Solu√ß√£o de Problemas

### Problema: "Table not found"
**Solu√ß√£o**: Execute as camadas em sequ√™ncia (Bronze ‚Üí Silver ‚Üí Gold)

```python
# Execute uma camada por vez
from src.bronze_layer import create_bronze_layer_job
from src.silver_layer import create_silver_layer_job
from src.gold_layer import create_gold_layer_job

# 1. Bronze
create_bronze_layer_job(spark, "/path/to/bronze")

# 2. Silver (ap√≥s Bronze completar)
create_silver_layer_job(spark, "/path/to/silver")

# 3. Gold (ap√≥s Silver completar)
create_gold_layer_job(spark, "/path/to/gold")
```

### Problema: Erro de download
**Solu√ß√£o**: Verificar conectividade e URLs da NYC TLC

```python
# Testar URLs manualmente
from src.bronze_layer import BronzeLayer
bronze = BronzeLayer(spark, "/tmp/bronze")
urls = bronze.get_data_urls(2023, [1])
print(urls[0])  # Verificar se a URL est√° acess√≠vel
```

### Problema: Erro de mem√≥ria
**Solu√ß√£o**: Ajustar configura√ß√µes do Spark

```python
# Aumentar recursos ou reduzir dados
spark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionNum", "1")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB")
```

##  Resultados Esperados

### An√°lise 1: M√©dia Mensal
```
Janeiro:  $16.52
Fevereiro: $17.34
Mar√ßo:    $16.89
Abril:    $17.12
Maio:     $16.97
```

### An√°lise 2: M√©dia Hor√°ria (Maio)
```
00:00: 1.45 passageiros
01:00: 1.38 passageiros
...
18:00: 1.62 passageiros (hora pico)
...
23:00: 1.51 passageiros
```

##  Checklist de Execu√ß√£o

- [ ] Ambiente Databricks configurado
- [ ] Arquivos uploaded
- [ ] Cluster Spark ativo
- [ ] Pipeline Bronze executado com sucesso
- [ ] Pipeline Silver executado com sucesso
- [ ] Pipeline Gold executado com sucesso
- [ ] An√°lises obrigat√≥rias executadas
- [ ] Resultados verificados e documentados

## üìû Suporte

Em caso de d√∫vidas:
1. Verifique os logs de execu√ß√£o
2. Consulte a documenta√ß√£o no README.md
3. Execute os testes: `python test_pipeline.py`
4. Verifique as configura√ß√µes de paths e permiss√µes

---

**Tempo estimado de execu√ß√£o**: 15-30 minutos (dependendo do ambiente)
**Dados processados**: ~5 meses de dados de t√°xis NYC (Jan-Mai 2023)
**Resultado**: An√°lises obrigat√≥rias + insights adicionais de neg√≥cio

